\documentclass[a4paper,12pt]{article}

%%% Работа с русским языком
\usepackage{cmap}					% поиск в PDF
\usepackage{mathtext} 				% русские буквы в формулах
\usepackage[T2A]{fontenc}			% кодировка
\usepackage[utf8]{inputenc}			% кодировка исходного текста
\usepackage[english,russian]{babel}	% локализация и переносы

%%% Дополнительная работа с математикой
\usepackage{amsfonts,amssymb,amsthm,mathtools} % AMS
\usepackage{amsmath}
\usepackage{icomma} % "Умная" запятая: $0,2$ --- число, $0, 2$ --- перечисление

\usepackage[left = 2cm, right = 2cm, top = 2cm, bottom = 2cm]{geometry}


%% Номера формул
%\mathtoolsset{showonlyrefs=true} % Показывать номера только у тех формул, на которые есть \eqref{} в тексте.

%% Шрифты
\usepackage{euscript}	 % Шрифт Евклид
\usepackage{mathrsfs} % Красивый матшрифт

%% Свои команды
\DeclareMathOperator{\sgn}{\mathop{sgn}}

%% Перенос знаков в формулах (по Львовскому)
\newcommand*{\hm}[1]{#1\nobreak\discretionary{}
	{\hbox{$\mathsurround=0pt #1$}}{}}

%%% Работа с картинками
\usepackage{graphicx}  % Для вставки рисунков
\graphicspath{{images/}{images2/}}  % папки с картинками
\setlength\fboxsep{3pt} % Отступ рамки \fbox{} от рисунка
\setlength\fboxrule{1pt} % Толщина линий рамки \fbox{}
\usepackage{wrapfig} % Обтекание рисунков и таблиц текстом

%%% Работа с таблицами
\usepackage{array,tabularx,tabulary,booktabs} % Дополнительная работа с таблицами
\usepackage{longtable}  % Длинные таблицы
\usepackage{multirow} % Слияние строк в таблице
\usepackage{upgreek}
\usepackage{enumerate}
\usepackage{ dsfont }

%%% Цветной текст

\usepackage[usenames]{color}
\usepackage{colortbl}
\usepackage[table,xcdraw]{xcolor}

%%% Солнышко

\usepackage[weather]{ifsym}

%%% Гиперссылки

\usepackage{xcolor}
\usepackage{hyperref}
\definecolor{linkcolor}{HTML}{199B03} % цвет ссылок
\definecolor{urlcolor}{HTML}{199B03} % цвет гиперссылок

\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}

\usepackage{minted}

%% Tikz

\usepackage{pgf,tikz,pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\pagestyle{empty}

%%Вставка картинок
\usepackage{graphicx}
\graphicspath{{pictures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}


%% эконометрические сокращения
\def \hb{\hat{\beta}}
\DeclareMathOperator{\sVar}{sVar}
\DeclareMathOperator{\sCov}{sCov}
\DeclareMathOperator{\sCorr}{sCorr}


\def \hs{\hat{s}}
\def \hy{\hat{y}}
\def \hY{\hat{Y}}
\def \he{\hat{\varepsilon}}
\def \v1{\vec{1}}
\def \cN{\mathcal{N}}
\def \e{\varepsilon}
\def \z{z}

\def \hVar{\widehat{\Var}}
\def \hCorr{\widehat{\Corr}}
\def \hCov{\widehat{\Cov}}

\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator*{\plim}{plim}


%% лаг
\renewcommand{\L}{\mathrm{L}}



% DEFS
\def \mbf{\mathbf}
\def \msf{\mathsf}
\def \mbb{\mathbb}
\def \tbf{\textbf}
\def \tsf{\textsf}
\def \ttt{\texttt}
\def \tbb{\textbb}

\def \wh{\widehat}
\def \wt{\widetilde}
\def \ni{\noindent}
\def \ol{\overline}
\def \cd{\cdot}
\def \bl{\bigl}
\def \br{\bigr}
\def \Bl{\Bigl}
\def \Br{\Bigr}
\def \fr{\frac}
\def \bs{\backslash}
\def \lims{\limits}
\def \arg{{\operatorname{arg}}}
\def \dist{{\operatorname{dist}}}
\def \VC{{\operatorname{VCdim}}}
\def \card{{\operatorname{card}}}
\def \sgn{{\operatorname{sign}\,}}
\def \sign{{\operatorname{sign}\,}}
\def \xfs{(x_1,\ldots,x_{n-1})}
\def \Tr{{\operatorname{\mbf{Tr}}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\amn}{arg\,min}
\DeclareMathOperator*{\amx}{arg\,max}
\def \cov{{\operatorname{Cov}}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}

\def \xfs{(x_1,\ldots,x_{n-1})}
\def \ti{\tilde}
\def \wti{\widetilde}


\def \mL{\mathcal{L}}
\def \mW{\mathcal{W}}
\def \mH{\mathcal{H}}
\def \mC{\mathcal{C}}
\def \mE{\mathcal{E}}
\def \mN{\mathcal{N}}
\def \mA{\mathcal{A}}
\def \mB{\mathcal{B}}
\def \mU{\mathcal{U}}
\def \mV{\mathcal{V}}
\def \mF{\mathcal{F}}

\def \R{\mbb R}
\def \N{\mbb N}
\def \Z{\mbb Z}
\def \P{\mbb{P}}
%\def \p{\mbb{P}}
\def \E{\mbb{E}}
\def \F{\mbb{F}}
\def \D{\msf{D}}
\def \I{\mbf{I}}
\def \L{\mathcal{L}}

\def \a{\alpha}
\def \b{\beta}
\def \t{\tau}
\def \dt{\delta}
\def \e{\varepsilon}
\def \ga{\gamma}
\def \kp{\varkappa}
\def \la{\lambda}
\def \sg{\sigma}
\def \sgm{\sigma}
\def \tt{\theta}
\def \ve{\varepsilon}
\def \Dt{\Delta}
\def \La{\Lambda}
\def \Sgm{\Sigma}
\def \Sg{\Sigma}
\def \Tt{\Theta}
\def \Om{\Omega}
\def \om{\omega}

%%% Заголовок
\author{Зехов Матвей}
\title{Заметки по многошаговому прогнозированию}
\date{\today}

\begin{document}
	
\newpage
\tableofcontents
\section{Домашнее задание 1}

\subsection{Задача 1}

\begin{figure}[h]
	
	\includegraphics[width = 0.5\linewidth]{11}
	\includegraphics[width = 0.5\linewidth]{12}
	\caption{Траектории}
	\label{traj}
\end{figure}	


\subsubsection{i}
Случайный процесс описан уравнением $ X_{t}=e^{\xi t} $

В зависимости от того, будет реализация случайной величины положительной или отрицательной, кривые будут либо экспоненциально возрастать, либо экспоненциально убывать, где $ \xi $ будет служить коэффициентом скорости роста. Чем ближе $ \xi $ к единице, тем быстрее будет возрастать кривая траектории, а чем ближе к минус единице, тем быстрее убывать. Соответственно, семейство кривых ограничено сверху кривой $ X_t = e^\xi $, а снизу -- кривой   $ X_t = e^{-\xi} $. Графики возможных траекторий можно увидеть на Рис. \ref{traj} слева.

Найдём конечномерные распределения процесса. Для простоты записи покажу на двумерном примере, а далее разширим до многомерного случая.

\begin{enumerate}[\Sun]
	\item Очевидно, что $ P\{X_1 \le x_1, X_2 \le x_2\} = 0 $ при $ x_1 \le 0 или х_2 \le $, так как показательнаяя функция от экспоненты не может быть отрицательной или нулевой.
	
	\item При $ x_1 \ge 1 \text{ и } x_2 \ge 1 $ :

	\begin{equation}
	\begin{aligned}
	 P\{X_1 \le x_1, X_2 \le x_2\}=  P\{e^{\xi t_1}& \le x_1, e^{\xi t_2} \le x_2\} = P\{\xi t_1 \le ln(x_1), \xi t_2 \le ln(x_2)\} =\\ P\{ \xi \le min\left(\frac{ln(x_1)}{t_1}, \frac{ln(x_2)}{t_2}\right) \} 
	 \end{aligned}
	 \end{equation}
	 
	 \item При $ x_1 \ge 1 \text{ и } 0 < x_2 < 1 $ :
	 
 	\begin{equation}
 	\begin{aligned}
	 P\{X_1 \le x_1, X_2 \le x_2\}=  P\{e^{\xi t_1}& \le x_1, e^{\xi t_2} \le x_2\} = P\{\xi t_1 \le ln(x_1), \xi t_2 \le ln(x_2)\} = \\  P\{ \xi \le min\left(\frac{ln(x_1)}{t_1}, \frac{ln(x_2)}{t_2}\right) \} 
	 \end{aligned}
	 \end{equation}
	 
	 Так как $ ln(x_2) $ , будет отрицательным, $ ln(x_1) $ - положительным, то $  P\{ \xi \le \frac{ln(x_2)}{t_2} \} $ будет ответом в данном случае. 
	 
	 \item При $ x_2 \ge 1 \text{ и } 0 < x_1 < 1 $ :
	 
	 Абсолютно аналогично предыдущему случаю
	 
	 \item При $ 0 < x_2 < 1 \text{ и } 0 < x_1 < 1 $ :
	 
	$  P\{X_1 \le x_1, X_2 \le x_2\}= \cdots  P\{ \xi \le min\left(\frac{ln(x_1)}{t_1}, \frac{ln(x_2)}{t_2}\right) \}  $
	
	В данном случае оба числа будут отрицательными и формула останется без сокращений.
	
	Очевидно (нет, ну правда очевидно, можно я не буду объяснять?), что в многомерном случае будет ровно то же самое. Следовательно, без потери общности, можно записать ответ в сокращённом виде:
	
	$ F_\xi(x_1, \cdots, x_n) = P\{X_1 \le x_1,\cdots, X_n \le x_n\} = 
	\begin{cases}
		0, \text{ если }  \exists j \text{ s.t. } x_j <= 0, j = 1:n \\
		F_\xi\left(min\left(\frac{ln(x_1)}{t_1},  \cdots, \frac{ln(x_n)}{t_n}\right) \right)
	\end{cases} $
	
	где $ F_\xi(x) $ - функция распределения равномерной случайной величины $ \xi $ на $ [-1, 1] $
	 
\end{enumerate}

\subsubsection{ii}
Случайный процесс описан уравнением $X_{t}=(\xi+\eta) / t$. В зависимости от того, будет ли реализация случайной величины $ \xi + \eta $ положительной или отрицательной, траекториями будут семейства гипербол. Соответственно, чем ближе к нулю будет реализована данная случайная величина, тем более вогнуты будут гиперболы вогнуты в сторону точки (0.0). Графики возможных траекторий можно увидеть на Рис. \ref{traj} справа.


Что же касается конечномерного распределения, то здесь всё довольно похоже на предыдущий пунктб поэтому напишу с минимумом подробностей. Решим для двумерного случая и расширим на многомерный.

 Для начала, однако, установим параметры нормального распределения случайной величины $ \xi + \eta $. Математическое ожидание ноль. Ковариация двух величин тоже ноль, так что дисперсия равна 1. Получим стандартную нормальную величину.

\[  P\{X_1 \le x_1, X_2 \le x_2\} = P\{ \xi + \eta \le x_1 t_1, \xi + \eta \le x_2 t_2 \} = P\{ \xi + \eta \le min(x_1 t_1, x_2 t_2) \} \]

Рассмотрим 4 случая:

\begin{enumerate}[\Sun]
	\item $ x_1 \le 0, x_2 \le 0 $
	
	В данном случае обе величины $ x_1 t_1, x_2 t_2 $ будут отрицательными и ответ будет: $ F_{N_{(0,1)}}(min(x_1 t_1, x_2 t_2)) $
	
	\item $ x_1 \ge 0, x_2 \ge 0 $ 
	Очевидно, обе величины $ x_1 t_1, x_2 t_2 $ будут положительными и ответ:$ F_{N_{(0,1)}}(min(x_1 t_1, x_2 t_2)) $
	
	\item $ x_1 > 0, x_2 < 0 $ 
	
	$ x_1 t_1 $ будет положительной величиной, а $ x_2 t_2 $ - отрицательной. Ответ: $ F_{N_{(0,1)}}(min(x_1 t_1, x_2 t_2)) = F_{N_{(0,1)}}(x_2 t_2) $
	
	\item $ x_1 < 0, x_2 > 0 $ 
		Ответ зеркален предыдущему. 
		
\end{enumerate}
Очевидно, что с повышением размерности ни один из этих вариантов не будет нарушаться. При наличии хотя бы одной отрицательной переменной $ x_j $ среди положительных, она автоматически станет минимумом, а при всех переменных одного знака формула и вовсе не упрощается. Следовательно, без потери общности, запишем ответ:

\[  	 F_\xi(x_1, \cdots, x_n) = P\{X_1 \le x_1,\cdots, X_n \le x_n\} =  F_{N_{(0,1)}}(min(x_1 t_1, \cdots, x_n t_n)) \]

\subsection{Задача 2}

\[ P\{X_{t_1} < X_{t_2}\} = P\{t_1(\xi_1 + \alpha(\xi_2 + 2\alpha))<t_1(\xi_1 + \alpha(\xi_2 + 2\alpha)) \}=  \]
\[
P\{ (t_1 - t_2)\xi_1 + (t_1 - t_2)\alpha \xi_2 < (t_2 - t_1)2\alpha^2\} = \\P\{ \xi_1 + \alpha\xi_2 \ge -2\alpha^2\} = P\{ \xi_1 + \alpha\xi_2 + 2\alpha^2 \ge 0\} = 1\]
	
	
Чтобы вероятность того, что эта случайная величина была положительной стала равной единице, рассмотрим график. Так как указано, что параметр $ \alpha $ является реальным числом, будем рассматривать только случаи с положительным дискриминантом. Чтобы учесть максимальное количество случаев, при которых значение функции в точке положительно, максимально "опустим" параболу, максимизировав дискриминант. Очевидно, что это произойдёт в двух точках относительно $ \xi $: (1, -1), (-1, -1)

\[ D = \xi_2^2 - 8\xi_1 \]

\[ \alpha_1 = \frac{-\xi_2 - \sqrt{\xi_2^2 - 8\xi_1}}{4} \]

\[ \alpha_1 = \frac{-\xi_2 + \sqrt{\xi_2^2 - 8\xi_1}}{4} \]


Все мозможные случаи корней при $ \xi_2 = +- 1 $:
\[ 
\begin{cases}
	\alpha_{11} = -\frac{1}{2}\\
	\alpha_{12} = -1\\
	\alpha_{21} = \frac{1}{2}\\
	\alpha_{22} = 1
\end{cases}
 \]

Следовательно, при $ \alpha \in [-1, 1] $ все возможные параболы будут принимать только неотрицательные значения.
Ответ:  $ \alpha \in [-1, 1] $ 

\subsection{Задача 3}

\[  f _ { z } ( x ) = \frac { 1 } { 2 } e ^ { - x } + e ^ { - 2 x } , x > 0  \]

\begin{equation}
\begin{aligned}  L [ p ] ( u ) = \int _ { 0 } ^ { \infty } \left( \frac { 1 } { 2 } e ^ { - x } + e ^ { - 2 x } \right) e ^ { - u x } d x  = \int _ { 0 } ^ { \infty } \frac { 1 } { 2 } e ^ { - x ( 1 + u ) } + e ^ { - x ( 2 + u ) } d x = \\ = \frac { 1 } { 2 ( 1 + u ) } + \frac { 1 } { 2 + u } = \frac { 2 + u + 2 + 2 u } { 4 + 4 u + 2 u + 2 u ^ { 2 } } =  \frac { 4 + 3 u } { 2 u ^ { 2 } + 6 u + 4 } \end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
L [ U ] ( u ) = \frac { \frac { 4 + 3 u } { 2 u ^ { 2 } + 6 u + 4 } } { u \left( 1 - \frac { 4 + 3 u } { 2 u ^ { 2 } + 6 u + 4 } \right) } = \frac { \frac { 4 + 3 u } { 2 u ^ { 2 } + 6 u + 4 } } { u \left( \frac { 2 u ^ { 2 } + 3 u } { 2 u ^ { 2 } + 6 u + 4 } \right) }= \frac { 3 u + 4 } { 2 u ^ { 3 } + 3 u ^ { 2 } } = \frac { 3 u  + 4 } { u ^ { 2 } ( 2 u + 3 ) } 
\end{aligned}
\end{equation}




\[ 
\frac { 3 u + 4 } { u ^ { 2 } ( 2 u + 3 ) } = \frac { A } { u } + \frac { B } { u ^ { 2 } } + \frac { C } { u + 3 }  = \frac { A u ( 2 u + 3 ) + B ( 2 u + 3 ) + C u ^ { 2 } } { u ^ { 2 } ( 2 u + 3 ) } = \frac { 2 A u ^ { 2 } + C u ^ { 2 } + 3 A u + 2 B u + 3 B } { u ^ { 2 } ( 2 u + 3 ) }
 \]

\begin{equation}
\left\{ \begin{array} { l } { C + 2 A = 0 } \\ { 3 A + 2 B = 3 } \\ { 3 B = 4 } \end{array} \Rightarrow \left\{ \begin{array} { l } { A = \frac { 1 } { 9 } } \\ { B = \frac { 4 } { 3 } } \\ { C = - \frac { 2 } { 9 } } \end{array} \right. \right.
\end{equation}


\[ U(t) = \frac{1}{9} + \frac{4}{3}t - \frac{1}{9}\exp^{-\frac{3}{2}t}\]

\subsection{Задача 4}
\subsubsection{i}

 \begin{wrapfigure}{l}{0.4\textwidth}
	\includegraphics[width=\linewidth]{13}
	\caption{$  f_{\xi - \eta}(x) $}
	\label{minmax}
\end{wrapfigure}


Для начала выведем несколько необходимых свойств функций плотностей.
\begin{equation}
\begin{aligned}
	F_{|\xi|} = P\{|\xi| \le x\}  = P\{-x \le \xi \le x \} =\\ P\{\xi \le x \} - P\{\xi \le -x \} = F_\xi(x) - F_\xi(-x) 
\end{aligned}
\end{equation}
Следовательно:
 \[ f_{|\xi|}(x) = f_\xi(x) + f_\xi(-x) \] 
 
 Теперь по формуле свёртки выведем следующую плотность:
 
 \[ f_{\xi - \eta}(x) = f_{\xi + (-\eta)}(x) \]
 
Для этого выведем следующее свойство:

 
\[  F_{-\eta}(x) = P\{-\eta \le x\} = P\{\eta \ge -x\} =\\ 1 -F_{\eta}(-x) \Rightarrow f_{-\eta}(x) = f_{\eta}(-x) \] 

Теперь возьмём интеграл:


\[  f_{\xi - \eta}(x) = \int_{-\infty}^{\infty} I\{u - x \in [0;1]\} I\{- x \in [0;1]\} = \int_{max(u-1,-1)}^{min(u,0)}1 dx = min(u,0) - max(u-1,-1) \]

Полученная фукнция изображена на \ref{minmax}.

Так как функция симметричная, $ f_{|\xi|}(x) = f_\xi(x) + f_\xi(-x) = 2 f_\xi(x) $. Следовательно: 

\[  f_{|\xi - \eta|}(x) = 2(min(u,0) - max(u-1,-1))  \]

Однако следует сделать важное замечание. Так как модуль случайной величины неотрицателен, складывать функции распределения следует только на положительной полуоси. Таким образом, ответ:

\[  f_{|\xi - \eta|}(x) = 
\begin{cases}
0, u < 0\\
2(min(u,0) - max(u-1,-1)), u \ge 0
\end{cases}
 \]
 
 Нетрудно проверить, что эта функция будет соответствовать всем необходимым свойствам функции плотности.


\subsubsection{ii}

По формуле свёртки:
\begin{equation*}
\begin{aligned}
f_{\xi + \eta} = \int_{-\infty}^{\infty} \frac{1}{4} \exp^{-|u-x|-|x|} dx=  \frac{1}{4}  \left( \int_{-\infty}^{0} \exp^{-|u-x|+x} dx + \int_{0}^{\infty}  \exp^{-|u-x|-x} dx \right) = \\ \frac{1}{4} \left( \int_{-\infty}^{min(u,0)} \exp^{-u+2x} dx+ \int_{min(u,0)}^{0} \exp^{u} dx+ \int_{0}^{max(u,0)}  \exp^{-u} dx + \int_{max(u,0)}^{+\infty}  \exp^{u-2x} dx \right) = \\
\frac{1}{4} \left( \frac{1}{2}\exp^{-u+2x} \Biggr|_{-\infty}^{min(u,0)}  + x \exp^u \Biggr|_{min(u,0)}^{0} + x \exp^{-u} \Biggr|_{0}^{max(u, 0)}  - \frac{1}{2} \exp^{u-2x} \Biggr|_{max(u, 0)}^{+\infty} \right) = \\
\frac{1}{4} \left( \frac{1}{2}\exp^{-u+2min(u,0)} - min(u,0) \exp^u + max(u,0) \exp^{-u} + \frac{1}{2} \exp^{u-2max(u, 0)} \right)
\end{aligned}
\end{equation*}

Вольфрам сказал, что интеграл под этой функцией равен единице, так что всё должно быть верно. По форме распределение напоминает нормальное. Касательно возникших функций минимума и максимума, они призваны регулировать функцию в зависимости от знака параметра $ u $. В зависимости от него один из четырёх интегралов во 2 строке будет схлопываться в нулевой.

\subsection{Задача 5}

\subsubsection{i}

Нет, не является процессом восстановления, так как $ p\{ \xi_i \ge 0 \} \neq 1 $

\subsubsection{ii}
Каждая траектория имеед вид ломаной кривой. Она начинается в точке ноль и образует один из путей (слева направо) в древовидной структуре на Рис.  \ref{rombs}.Для примера одна из возможных траекторий окрашена в оранжевый. Данная фигура по виду очень напоминает треугольник Паскаля.


\begin{figure}[h]
	\includegraphics[width=0.3\linewidth]{14}
	\caption{Траектории $ S_n $}
	\label{rombs}
\end{figure}
\subsubsection{iii}

Сколько-нибудь адекватный ответ в явном виде у меня не получился, остался только следующий вариант:

\begin{equation*}
\begin{aligned}
P\{  X_1 \le x_1, X_2 \le x_2, \cdots, X_n \le x_n \}  = P\{  X_1 \le x_1, X_2 \le x_2, \cdots, X_n \le x_n \} =\\
P\{  \sum_{i = 1}^{t_1}\xi_i \le x_1, \sum_{i = 1}^{t_2}\xi_i \le x_2, \cdots, \sum_{i = 1}^{t_n}\xi_i \le x_n \} = \\ P\{  \sum_{i = 1}^{t_1}\xi_i \le x_1, \sum_{i = t_1 + 1}^{t_2}\xi_i \le x_2 - x_1, \cdots, \sum_{i = t_{n-1}+1}^{t_n}\xi_i \le x_n - x_{n-1} \}
\end{aligned}
\end{equation*}




Логика такого перехода в следующем:

\[ \sum_{t_1 + 1}^{t_2}\xi_i + x_1 \le \sum_{1}^{t_2} \xi_i \le x_2 \Rightarrow \sum_{t_1 + 1}^{t_2} \xi_i \le x_2 - x_1\]

Нетрудно проверить, что для каждого периода необходимо просто вычитать предыдущий. Я не доконца уверен в этом переходе, но выглядит красиво. Теперь события независимы. Можно разбить на произведение свёрток в смысле распределений:


\begin{equation*}
\begin{aligned}
 P\{  \sum_{i = 1}^{t_1}\xi_i \le x_1, \sum_{i = t_1 + 1}^{t_2}\xi_i \le x_2 - x_1, \cdots, \sum_{i = t_{n-1}}^{t_n}\xi_i \le x_n - x_{n-1}\} =&\\ F^{*t_1}(x_1) \cdot F^{*(t_2 - t_1)}(x_2 - x_1) \cdot \cdots \cdot F^{*(t_n - t_{n-1})}(x_n - x_{n-1})
\end{aligned}
\end{equation*}

Единственное ограничение, которое можно наложить на переменные, это что при  $ x_j - x_{j-1}  < t_j - t_{j-1}$ выражение $ \sum_{i = t_{j-1} + 1}^{t_j}\xi_i $ обратится в ноль. Это случится потому что сумма описанных выше величин не может быть мешьше чем (-1) * (количество величин в сумме). Итоговый ответ можно записать следующим образом:

$ F_\xi(x_1, \cdots, x_n) =
\begin{cases}
	0, \text{ если } \exists \text{ j s.t. } x_j - x_{j-1}  < t_j - t_{j-1} \\
 F^{*t_1}(x_1) \cdot F^{*(t_2 - t_1)}(x_2 - x_1) \cdot \cdots \cdot F^{*(t_n - t_{n-1})}(x_n - x_{n-1}) \text{ иначе }
\end{cases}
$

Мне самому не очень нравится этот ответ, так как она не даёт идей для следующего пункта и так как эти непонятные свёртки вообще неясно как брать в случае дискретных величин. 


\subsubsection{iv}

\subsection{Задача 6}

Данное утверждение неверно. (Иначе бы его дали в лекции как более общее, ну логично же)

Событие $ \{N_t \le n\}$ можно интерпретировать следующим образом. Возможны три варианта событий:

\begin{enumerate}[\Sun]
	\item	К моменту времени $ t $ появилось менее n клиентов. 
	
	\item В момент времени $ t $ подошёл $ n $-ый покупатель.
	
	\item В какой-то из моментов времени до $ t $ подошёл $ n $-ый покупатель, и вплоть до момента $ t $ более покупателей не приходило
\end{enumerate}

Следовательно:
\[  \{N_t \le n\} = \{S_n > t\} \cup \{S_n = t\} \cup \{S_n < t\} \neq \{S_n \ge t \} =  \{S_n > t\} \cup \{S_n = t\}  \]

Исходное утверждение неверно.


\section{Домашнее задание 2}

\subsection{Задача 1}

Начальное условие: $ Z_0 = c $

Обозначим случайную величину $ \tau $ следующим образом: 

\[\tau = \begin{cases}
1,  1-F_\eta(R)\\
0, F_\eta(R)
\end{cases}  \]

Пусть $ \E(\xi_n) = \mu $

Процесс восстановления: $ Z_n = Z_{n-1} + \tau_n \xi_n $

Вычтем начальное условие из обоих частей:

\[ Z_n - c = Z_{n-1} - c + \tau_n \xi_n \]

Переобозначим:

\[  S_n = S_{n-1} + \tau_n \xi_n  \]

$ N_t = max\{ k, S_k \le t \} = max\{ k, Z_k - c \le t \} = max\{ k, Z_k \le t + c\} = M(C) $

\[ t + c = C \Rightarrow t = C - c  \]

\[ \lim\limits_{t \to \infty} \frac{N_t}{t} = \frac{1}{\E(\tau \xi_n)} = \frac{1}{(1 - F_\eta(R))\mu} = \lim\limits_{C \to \infty} \frac{M(C)}{t(C)} \]

\[ \E(\tau \xi_n) = \text{ независимость } = (1 - F_\eta(R)) \mu \]


\[ \lim\limits_{C \to \infty} \frac{M(C)}{t(C)} =  \frac{1}{(1 - F_\eta(R))\mu} \Rightarrow  \lim\limits_{C \to \infty} M(C) = \frac{C - c}{(1 - F_\eta(R))\mu} \]

\subsection{Задача 2} 

\subsubsection{i}
 
 Это событие будет подчиняться геометрическому распределению. По всем известной формуле математического ожидания это будет $ \frac{1}{p} $

\subsubsection{ii}

Процесс восстановления: $ S_n = S_{n-1} + \xi_n $

$ \E \xi_i = 45 $

Обозначим индикатор обнаружения :

\[\tau = \begin{cases}
1,  p\\
0, 1-p
\end{cases}  \]

Штраф: $ \zeta \sim U[0, C(\frac{A}{B})],  $

Вознаграждение случайного процесса:  $ R_i = \tau \zeta  \Rightarrow \text{ независимость } \Rightarrow \E(R_i) = \frac{p C}{2}$

\[ \frac{Y(t)}{t} \to \frac{p C}{90} \Rightarrow Y(t) \to \frac{\tau p C}{90}\]


\subsubsection{iii}

Рассмотрим две альтернативы поведения. Первый вариант поведения владельца это экономия. Усредним возможные профиты и лоссы. В таком случае в любой конкретный день он в среднем будет получать профит $ A - B $. Константу сколько не усредняй, останется константой. Однако он будет в среднем получать асимптотическиий штраф  $ Y(t) \to \frac{\t p C}{90} $, который мы вычислили в предыдущем пункте.

В ином вариант, когда владелец выбирает не экономить, он не получает выгоды, но в среднем каждый день теряет $ А $ рублей.

 В таком случае владельцу будет выгодно экономить, если средяя "чистая прибыль" от экономии будет больше, чем от экономии, то есть:
 
 \[ A - B - \frac{Y(t)}{t} > -A \]
 
 \[ A - B - \frac{p C}{90} > -A \]
 
 В таком случае владельцу будет выгодна первая стратегия даже если чистая прибыль от экономии будет отрицательной из-за штрафов, но будет больше чем $ -A $, то экономия всё равно останется оптимальной. Преобразуя неравенство, получим:
 
 \[  2A - B - \frac{pC}{90} > 0 \Rightarrow \frac{90(2A-B)}{C(\frac{A}{B})} > p\]
 
 Если честно, я не понял, как использовать зависимость $ С $ от дроби. Разве что наложить дополнительные условия на производную $ C $ по $ A $ и $ B $. Возможно это даст какие-то дополнительные условия на $ C $, но особого смысла в этом не вижу.
 
 \subsection{Задача 3}
 
 Выпишем суммарное вознаграждение процесса востановления. Для начала обозначим пару вспомогательных индикаторов. $ \tau $ -- индикатор того, что ремонт возможно произвести самостоятельно. $ \rho $ -- индикатор того, что самостоятельный ремонт был некачественным. \
 
 \[\tau = \begin{cases}
 1,  p\\
 0, 1-p
 \end{cases}  \]
 
  \[\rho = \begin{cases}
 1,  q\\
 0, 1-q
 \end{cases}  \]
 
 \[ R_i = \tau \rho (m + \eta) + \tau (1-\rho)m + (1 - \tau) \eta \]
 

\subsubsection{i}

В данном пункте необходимо только первое слагаемое. При $  t \to \infty $ уммарные расходы будут следующими:

\[ \frac{Y(t)}{t} \to \frac{\E(R_i^I)}{\E(\xi_i)} = \text{независимость} = \frac{pq\left(m + \frac{M + m}{2}\right)}{18} \Rightarrow Y(t) \to \frac{tpq\left(m + \frac{M + m}{2}\right)}{18}  \]

\subsubsection{ii}

Сравним ожидаемые вознаграждения за самостоятельный ремонт и за ремонт в автосервисе. Первое должно быть меньше второго. По-хорошему, нужно обе части неравенства ниже делить на $ \E(xi_i) $б но все понимают, что я просто мысленно на это же положительное число 18 просто домножил обе части чтобы лишние дроби не тянуть. Матожидания позволю себе также вычислить в уме.

\begin{equation}
\begin{aligned}
pq\left(m + \frac{M + m}{2}\right) + p(1-q)m < \frac{(1-p)(M+m)}{2} \Rightarrow \Big| *2 \text{ и } :q\\
q(M + 3m) + 2m(1-q) < \frac{M+m}{p} - (M + m) \Rightarrow \Big| :(M+m) \\
\frac{q(M + 3m) + 2m - 2qm}{M+m} < \frac{1 - p}{p} \Rightarrow
\frac{qM + qm + 2m}{M + m} < \frac{1 - p}{p} \Rightarrow \\
q + \frac{2m}{M + m} < \frac{1 - p}{p} 
\end{aligned}
\end{equation}

\subsection{Задача 4}

\begin{figure}[h]
	
	\includegraphics[width = 0.5\linewidth]{21}
	\includegraphics[width = 0.5\linewidth]{22}
	\label{police}
\end{figure}

Как и в лекции, будем пользоваться теоремой о двух милиционерах. Это до ужаса скучно, но так и быть. Поправка к графикам, которые у меня уже нет сил перерисовывать: по оси ординат, конечно же, $ \xi_1, \xi_2 ... $? а не $ S_1, S_2 ... $

\subsubsection{i}

Функция под интегралом представляет собой просто куски прямой $ Z(t) = t $, которя в каждый момент восстановления просто сдвигается на $ \xi_i $.
Как видно из графика слева на Рис. \ref{police}, искомый интеграл ограничен суммами площадей треугольников до точек $ N_t $ и $ N_t+1 $. Найдём пределы границ неравенства.

\[ \frac{\sum_{1}^{N_t}\frac{1}{2}\xi_i^2}{t} \le \int_{0}^{t}Z_u^{w} du\le \frac{\sum_{1}^{N_t + 1}\frac{1}{2}\xi_i^2}{t} \]

\[ \lim\limits_{t \to \infty}\frac{1}{2t} \sum_{1}^{N_t}\xi_i^2 = \lim\limits_{t \to \infty} \frac{N_t}{t}  \frac{\sum_{1}^{N_t}\xi_i^2 }{2 N_t} = \frac{\E(\xi_1^2)}{2 \E(\xi_1)} \]


\[ \lim\limits_{t \to \infty}\frac{1}{2t} \sum_{1}^{N_t + 1}\xi_i^2 = \lim\limits_{t \to \infty}\frac{1}{2t} \sum_{1}^{N_t + 1}\xi_i^2 \frac{N_t + 1}{N_t}  \frac{N_t}{N_t+1}= 
\lim\limits_{t \to \infty} \frac{N_t}{t}  \frac{\sum_{1}^{N_t}\xi_i^2 }{2 N_t} \frac{N_t + 1}{N_t} = \frac{\E(\xi_1^2)}{2 \E(\xi_1)} \]

Видим, что исходная функция зажата двумя абсолютно идентичными функциями. Следовательно, по теореме о двух милиционерах предел исходной функции тоже будет равен $ \frac{\E(\xi_1^2)}{2 \E(\xi_1)} $




\subsubsection{ii}

Пункт абсолютно идентичен предыдущему. Единственная разница лишь в построении графика. Искомое время является ни чем иным как $ \xi_{N_t+1} $. Скачки графика происходят непосредственно в моменты восстановления. Все вычисления и выводы абсолютно идентичны, с поправкой на $ \frac{1}{2} $, так как площадь каждого квадрата будет ровно $ \xi_i^2 $.


\[ \frac{\sum_{1}^{N_t}\xi_i^2}{t} \le \int_{0}^{t}V_u^{w} du\le \frac{\sum_{1}^{N_t + 1}\xi_i^2}{t} \]

\[ \lim\limits_{t \to \infty} \sum_{1}^{N_t}\xi_i^2 = \lim\limits_{t \to \infty} \frac{N_t}{t}  \frac{\sum_{1}^{N_t}\xi_i^2 }{N_t} = \frac{\E(\xi_1^2)}{\E(\xi_1)} \]

\[ \lim\limits_{t \to \infty} \sum_{1}^{N_t + 1}\xi_i^2 = \lim\limits_{t \to \infty} \sum_{1}^{N_t + 1}\xi_i^2 \frac{N_t + 1}{N_t}  \frac{N_t}{N_t+1}= 
\lim\limits_{t \to \infty} \frac{N_t}{t}  \frac{\sum_{1}^{N_t}\xi_i^2 }{ N_t} \frac{N_t + 1}{N_t} = \frac{\E(\xi_1^2)}{ \E(\xi_1)} \]

Видим, что исходная функция зажата двумя абсолютно идентичными функциями. Следовательно, по теореме о двух милиционерах предел исходной функции тоже будет равен $ \frac{\E(\xi_1^2)}{ \E(\xi_1)} $


\subsection{Задача 5}

\subsection{Задача 6}

\subsubsection{i}

\[  \E(S_{N_t+1}) = \mu \E(N_t) + \mu\]

Далее сделаем ключевой переход. $ S_{N_t+1} $ это точка времени, в которую произойдёт следующий после точки $ t $  эпизод восстановления. Очевидно, что математическое ожидание этой случайной величины больше $ t $, так как это событие должно произойти после $ t $. Следовательно:

\[  \E(S_{N_t+1}) = \mu \E(N_t) + \mu > t \Rightarrow \E(N_t) > \frac{t}{\mu} - 1 \Rightarrow \frac{\E(N_t)}{t} > \frac{1}{\mu} - \frac{1}{t}\]

\subsubsection{ii}

Снова воспользуемся тождеством Вальда. Начём доказывать с конца.

\[ \E(\tilde{N}_t)  \le \frac{t}{\tilde{\mu}(\sqrt{t})} + \frac{\sqrt{t}}{\tilde{\mu}(\sqrt{t})} \Rightarrow  \tilde{\mu}(\sqrt{t}) \E(\tilde{N}_t) \le t + \sqrt{t} \]

Согласно тождеству Вальда:

\[ \E(S_{N_t}) =  \tilde{\mu}(\sqrt{t}) \E(\tilde{N}_t) \]


Следовательно:
\[  \E(S_{N_t}) \le t + \sqrt{t}  \]

Данное неравенство выполняется всегда, так как событие $ S_{N_t} $ -- последний момент восстановлления до $ t $, и его математическое ожидание должно быть меньше $ t $. Следовательно, получаем тождество. Исходное предположение доказано.

Что же касается левой части неравенства, её можно доказать интуитивно. Так как в процессе восстановления в приращениях всегда будет прибавляться меньший чем $ \xi_n $ отрезок времени $ \tilde{\xi}_n $, то до момента времени $ t $ произойдёт точно не меньше эпизодов восстановлени (если все реализации случайной величины $ \xi_n $ будут больше $ b $) или больше. Следовательно, математическое ожидание количества восстановлений к моменту $ t $ тоже будет выше. 

Оба положения неравенства доказаны.

\subsubsection{iii}

После первых двух пунктов получаем неравенство:

\[ \frac{1}{\mu} - \frac{1 }{t}  < \frac{\E(N_t)}{t} < \frac{1}{\tilde{\mu}(\sqrt{t})} +\frac{1}{\sqrt{t}\tilde{\mu}(\sqrt{t})} \]

Теперь, очевидно, как и в задаче 4, нужно воспользоваться теоремой о двух милиционерах. Но сначала нужно доказать, что $\tilde{\mu}(\sqrt{t}) \rightarrow \mu \operatorname{ при } t \rightarrow \infty$

Для этого нужно вычислить следующее:

\[  \lim\limits_{t \rightarrow +\infty}   \E(min(\sqrt{t}, \xi_n)) \]

Для этого так и напрашивается поменять местами предел и математическое ожидание. Однако для этого нужно выполнить условия Dominated convergence theorem. Как бы по-хорошему нужно выписать все предпосылки о вероятностном пространстве как метрическом пространстве и обозначить предпосылки, но сил уже на это мало. Обозначим самые главные. Нужно найти такую мажорирующую функцию $ g $, что:

\begin{enumerate}[\Sun]
	\item Функция плотности $ g $ должна быть интегрируема
	
	\item Математическое ожидание модуля $ g $ конечно
	
	\item Функция $ g  $ должна доминировать исходную функцию.
\end{enumerate}

Всё просто. Обозначим $ g = \xi_n $. Её математическое ожидание конечно по условию, и мы можем менять в исходном неравенстве предел и математическое ожидание.

Можно проиллюстрировать всё следующим примером.

Очевидно, что:
\[ min(b, \xi_n) \le \xi_n\]

Это было как раз условие доминирования. Оно верно с учётом того, что $  \xi_n $ неотрицательная случайная величина. Домножим на неотрицательную функцию плотности.

\[f_\xi(x) min(b, \xi_n) \le f_\xi(x)\xi_n \]

Возьмём математическое ожидание обеих частей:

\[ \int_{0}^{+\infty} f_\xi(x) min(\sqrt{t}, x) dx \le \int_{0}^{+\infty} f_\xi(x) x dx \]


Математическое ожидание исходной функции тоже доминировано конечным математическим ожиданием $ \xi $

По пунктам. Нужно ввести предпосылку о том, что функция плотности $ \xi $ интегрируема. Математическое ожидание модуля $ \xi $ равно математическому ожиданию $ \xi $ и конечно. Очевидно, что $ \xi_n $  доминирует исходную функцию.

Поменяем предел и математическое ожидание:

\[  \lim\limits_{t \rightarrow +\infty}   \E(min(\sqrt{t}, \xi_n)) \Rightarrow    \E( \lim\limits_{t \rightarrow +\infty}  min(\sqrt{t}, \xi_n)) = \E(\xi_n)  = \mu\]


Следовательно, мы доказали, что $\tilde{\mu}(\sqrt{t}) \rightarrow \mu \operatorname{ при } t \rightarrow \infty$. Теперь воспользуемся-таки теоремой о двух милиционерах и возьмём пределы по двум границам исходного неравенства:


\[ \frac{1}{\mu} - \frac{1 }{t}  < \frac{\E(N_t)}{t} \le \frac{1}{\tilde{\mu}(\sqrt{t})} +\frac{1}{\sqrt{t}\tilde{\mu}(\sqrt{t})} \]

Очевидно, что при $ t \to +\infty $ дроби с $ t $ в знаменателях занулятся, а в правой части по доказанной выше сходимости появится тоже $ \nu $ В итоге:

\[ \frac{1}{\mu}   <  \lim\limits_{t \rightarrow +\infty} \frac{\E(N_t)}{t} \le \frac{1}{\mu}  \]

Следовательно, получаем: 

\[  \lim\limits_{t \rightarrow +\infty} \frac{\E(N_t)}{t} = \frac{1}{\mu}    \]


\section{Домашнее задание 3}

\subsection{Номер 1}
\subsubsection{i}

Докажем по индукции. Начальное условие:

\[ S_1 = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x} \]

Индукционный переход:

\[ n \to n+1, S_{n+1} = S_n + \xi_{n+1} \]

\begin{equation}
\begin{aligned}
f_{S_{n+1}}(x) = \int_{\R+} f_{S_n}(x-y)f_{\xi_{n+1}}(y)dy = & \int_{\R+} \frac{\beta^{n\alpha}}{\Gamma(n\alpha)} x^{n\alpha-1} e^{-\beta x} I\{x-y>0\} \cdot \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x} I\{y>0\} dy =\\
\frac{\beta^{(n+1)\alpha}}{\Gamma(n\alpha) \Gamma(\alpha)} e^{\beta x} \underbrace{ \int_{0}^{x} (x-y)^{n\alpha - 1} y^{\alpha-1} dy}_{I} =  \dots
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
I = \int_{0}^{x} (x-y)^{n\alpha - 1} y^{\alpha-1} dy = |z = \frac{y}{x}| = \int_{0}^{1} (x - zx)^{n\alpha - 1}zx^{\alpha-1} x dz =&\\ = \text{По свойству Бета-функции} = x^{(n+1)\alpha - 1} \frac{\Gamma(n\alpha) \Gamma(\alpha)}{\Gamma((n+1)\alpha)}
\end{aligned}
\end{equation}


\begin{equation}
\begin{aligned}
\dots = \frac{\beta^{(n+1)\alpha} x^{(n+1) \alpha - 1}}{\Gamma((n+1)\alpha)} e^{-\beta x} = f_{S_{n+1}}(x)
\end{aligned}
\end{equation}

\subsubsection{ii}

\[ P\{N_t = n\} = P\{S_n \le t\} - {\{ S_{n+1} \le t \}} \]


\begin{equation}
\begin{aligned}
F_{S_n}(x) = \int_{0}^{t} \frac{\beta^{n\alpha}}{\Gamma(n\alpha)} x^{n\alpha-1} e^{-\beta x} dx
\end{aligned}
\end{equation}

\begin{equation}
\begin{aligned}
 P\{N_t = n\} = \int_{0}^{t} \frac{\beta^{n\alpha}}{\Gamma(n\alpha)} x^{n\alpha-1} e^{-\beta x} dx - \int_{0}^{t} & \frac{\beta^{(n+1)\alpha}}{\Gamma((n+1)\alpha)} x^{(n+1)\alpha-1} e^{-\beta x} dx = \\
 \int_{0}^{t} \beta^{n\alpha} x^{n\alpha-1} e^{-\beta x} (\frac{1}{\Gamma(N\alpha)} - \frac{\beta^\alpha}{\Gamma((n+1)\alpha)}x^\alpha) 
\end{aligned}
\end{equation}

Я не знаю, как дальше брать этот интеграл кроме как численно. Ну может тут тоже есть какой-то финт ушами через дискретную вариацию распределения аля Эрланг, но я не придумал, как его тут применить. Оставлю тут просто солнышко. Вот оно: \Sun


\subsection{Номер 2}

Предположим независимость случайных величин $ S_2 $ и $ \gamma $ (суммарное время обслуживания первого клиента). Это тонкий момент. Я так и не смог привести контрпример к независимости. Вопрос в том, можно ли отделить время прихода первого покупателя от времени стрижки. В таком случае простое ручное вычисление $ \Cov(\xi_1 + \xi_2, \xi_2 + \eta) $ Даст как минимум $ \Var(\xi_1) $. Следственно, предположим, что вторая величиина неделима и независима от $ \xi_1 $. Иначе задача нерешаема в текущих условиях.

Где необходимо, будем пользоваться следующим утверждением (очевидно, по Лопеталю):
\[ \lim\limits_{x \to +\infty} x e^{-x} = 0 \]


\begin{equation}
\begin{aligned}
P\{ \xi_1 + \xi_2 < \gamma \} = \iint_{x_1 < x_2} \lambda_1^2 x_1 e^{-\lambda_1x_1} \lambda_2 e^{-\lambda_2x_2 } dx_1 dx_2 = 
\lambda_1^2\lambda_2 \int_{0}^{+ \infty} \int_{x_1}^{+ \infty} x_1 e^{-(\lambda_1x_1 + \lambda_2 x_2 )} dx_2 dx_1 = \\
\lambda_1^2\lambda_2 \int_{0}^{+ \infty} \left( -\frac{x_1 e^{-(\lambda_1x_1 + \lambda_2 x_2 )}}{\lambda_2} \bigg|^{+ \infty}_{x_1} \right) dx_1= \lambda_1^2\lambda_2 \int_{0}^{+ \infty} \left( \frac{x_1 e^{-(\lambda_1 + \lambda_2) x_1}}{\lambda_2} \right) dx_1 = \\ \lambda_1^2 \int_{0}^{+ \infty} \left( x_1 e^{-(\lambda_1 + \lambda_2) x_1} \right) dx_1 = \lambda_1^2 \left( \frac{x_1 e^{-(\lambda_1 + \lambda_2) x_1}}{-(\lambda_1 + \lambda_2)} \bigg |^{+\infty}_0 - \int_{0}^{+ \infty} \frac{ e^{-(\lambda_1 + \lambda_2) x_1}}{-(\lambda_1 + \lambda_2)} dx_1 \right) = \\ - \lambda_1^2 \left( \int_{0}^{+ \infty} \frac{ e^{-(\lambda_1 + \lambda_2) x_1}}{-(\lambda_1 + \lambda_2)} dx_1 \right)
= -\lambda_1^2 \left(  \frac{ e^{-(\lambda_1 + \lambda_2) x_1}}{(\lambda_1 + \lambda_2)^2}\bigg |^{+\infty}_0 \right) =
\frac{\lambda_1^2}{(\lambda_1 + \lambda_2)^2} = \frac{\frac{1}{100}}{(\frac{35}{250})^2} \approx 0.51
\end{aligned}
\end{equation}


\subsection{Номер 3}

\subsection{Номер 4}

Необходимо найти следующую вероятность: 

\[ P\{ N_{30} \le 3 | N_{10} \ge 2 \} \]

Эту вероятность можно расписать по формуле полной вероятности. Это событие реализуется при трёх возможных условиях:

\[ N_t = 2, N_t = 3, N_t \ge 4 \]

Важно отметить, что $ (A|N_t = 2 | N_t \ge 2) = (A|N_t = 2)) $. Первый элемент уравнения -- событие, на которое наложены сразу два условия. Вероятности условий также должны стать условными событиями. Только тогда их сумма будет равняться единице.



В таком случае, запишем формулу следующим образом:

\begin{equation}
\begin{aligned}
 P\{ N_{30} \le 3 | N_{10} \ge 2 \}  = P\{ N_{30} \le 3 | N_{10} = 2 \} P\{ N_{10} = 2 | N_{10} \ge 2 \} + & \\ P\{ N_{30} \le 3 | N_{10} = 3 \} P\{ N_{10} = 3 | N_{10} \ge 2 \} + P\{ N_{30} \le 3 | N_{10} \ge 4 \} P\{ N_{10} \ge 4 | N_{10} \ge 2 \} = & \\(P\{ N_{30} - N_{10} = 0 \} + P\{ N_{30} - N_{10} = 1 \}) P\{ N_{10} = 2 | N_{10} \ge 2 \} + P\{ N_{30} - N_{10} = 0 \} P\{ N_{10} = 3 | N_{10} \ge 2 \}
\end{aligned}
\end{equation}

Найдём все необходимые нам вероятности. Для этого воспользуемся доказанным на лекции утверждением:

\[ N_t - N_s \sim Pois(\lambda(t - s)) \forall t,s \ge 0, t > s \]

Следовательно:

\[ P\{ N_t - N_s = k \} = \frac{(\lambda(t-s))^k}{k!}e^{-\lambda(t-s)}\]



При $ t - s = 5 $ получим математическое ожидание процесса приращений равное одной квартире. Соответственно, параметр интенсивности такого распределения вычисляется следующим образом:
\[  \lambda(t - s) = 1 \Rightarrow \lambda = \frac{1}{t-s} \Rightarrow \lambda = \frac{1}{5}\]


\[ P\{N_{10} = 0 \} =  e^{-2}  \approx 0.14\]

\[ P\{N_{10} = 1 \} =  2e^{-2}  \approx 0.28\]

\[ P\{N_{10} = 2 \} = \frac{(\frac{1}{5} 10)^2}{2!} e^{-2}  \approx 0.27\]

\[ P\{N_{10} = 3 \} = \frac{(\frac{1}{5} 10)^3}{3!} e^{-2}  \approx 0.18\]

\[ P\{N_{10} \ge 4 \} =1 -0.14 -0.28 - 0.27 - 0.18 =0.13 \]



\[ P\{ N_{30} - N_{10} = 0 \} = \frac{1}{0!} e^{-\frac{1}{5}20} \approx 0.018\]

\[ P\{ N_{30} - N_{10} = 1 \} = \frac{\frac{1}{5}(30-10)}{1!} e^{-\frac{1}{5}20} = 4 e^{-4} \approx 0.073\]

\[ P\{ N_{10} = 2 | N_{10} >= 2\} = \frac{0.27}{0.27 + 0.18 + 0.13} \approx 0.465 \]

\[ P\{ N_{10} = 3 | N_{10} >= 2\} = \frac{0.18}{0.27 + 0.18 + 0.13} \approx 0.31 \]

\[ P\{ N_{10} \ge 4 | N_{10} >= 2\} = \frac{0.13}{0.27 + 0.18 + 0.13} \approx 0.22 \]


Засунем всё обратно в формулу:

\begin{equation}
\begin{aligned}
P\{ N_{30} \le 3 | N_{10} \ge 2 \}  = (0.018 + 0.073) \cdot 0.465 + 0.018 \cdot 0.31 = 0.0423 + 0.00558 = 0.04788
\end{aligned}
\end{equation}

\section{Домашнее задание 4}

\subsection{Задача 1}

\subsubsection{i}
\[ \E(e^{iu(2\xi - \I\{ \xi > \frac{1}{2} \})}) \]

Определим распределение величины  $ 2\xi - \I\{ \xi > \frac{1}{2} \}
 $, разложив её по формуле полной вероятности и использовав формулу полной вероятности.
 
\begin{equation}
\begin{aligned}
 \P\{  2\xi - \I\{ \xi > \frac{1}{2}\}  \le x \} = \P\{ 2\xi \le x | \xi \le \frac{1}{2} \}\P\{  \xi \le \frac{1}{2} \}+ \P\{ 2\xi - 1  \le x |  \xi > \frac{1}{2} \} \P\{  \I\{ \xi > \frac{1}{2} \} = \\ \frac{\P\{ 2\xi \le x \cap \xi \le \frac{1}{2} \}}{\P \{ \xi \le \frac{1}{2} \}} \P\{  \xi \le \frac{1}{2} \} +\frac{\P\{ 2\xi - 1 \le x \cap \xi > \frac{1}{2} \}}{\P \{ \xi > \frac{1}{2} \}} \P\{  \xi > \frac{1}{2} \} =\\ \P\{ 2\xi \le x \cap \xi \le \frac{1}{2} \} + \P\{ 2\xi - 1 \le x \cap \xi > \frac{1}{2} \} = \F_\xi(min(\frac{x}{2}, \frac{1}{2})) + \F_\xi(\frac{x + 1}{2}) - \F_\xi(\frac{1}{2}) = \F_\xi(\frac{x}{2}) + \F_\xi(\frac{x}{2}) =\\ \frac{x}{2} + \frac{x}{2} = x \text{ при } x\in[0;1]
\end{aligned}
\end{equation}

Последние переходы поясню. Минимум из $ \frac{x}{2} $ и $ \frac{1}{2} $ всегда будет в пользу первого варианта, так как $ x \in [0;1] $. В следствие линейности равномерной функции распределения очевидно, что $ \F_\xi(\frac{x + 1}{2}) =  \F_\xi(\frac{x}{2}) + \F_\xi(\frac{1}{2}) $

Получается, что $ \eta_1 \sim U[0,1] $. Следовательно, характеристическая функция имеет следующий вид:

\[ \E e^{iu\eta_1} = \int_{0}^{1} e^{iux} dx =\frac{ e^{iux} }{iu}\biggr|_0^1 = \frac{e^{iu} - 1}{iu} = \frac{cos(u) + i sin(u) - 1}{iu}\]

Очевидно, ответ будет комплексным.


\subsubsection{ii}

\[ \E e^{iuln(\xi)}  = \E (e^{ln(\xi)})^{iu} = \E \xi^{iu} = \int_{0}^{1} x^{iu} dx =\frac{ x^{iu+1}}{iu+1} \biggr|_0^1 = \frac{1^{iu+1}}{iu+1} = \frac{ 1^{iu}}{iu+1}\]

Если расписать единичу в степени мнимой единицы по общей формуле комплексной степени комплексного числа, получим:

\[\begin{aligned}\left(r e^{i \theta}\right)^{z} &=\exp \{z(\ln r+i \theta+2 i k \pi)\} \\ &=\exp \{z(\ln r+i \theta)\} \cdot \exp \{2 i k \pi \cdot z\} \end{aligned}\]

Получим: $ 1^i = e^{2\pi k} $

Числитель действительнозначный, знаменатель комплексный. Ответ комплексный.


\subsubsection{iii}

$\eta_{3}=\left\{\begin{array}{ll}{-1,} & {0 \leq \xi<1 / 3} \\ {0,} & {1 / 3 \leq \xi<2 / 3} \\ {1,} & {2 / 3 \leq \xi \leq 1}\end{array}\right.$



\begin{equation}
\begin{aligned}
 \E e^{iu\eta_3}  = \frac{1}{3} (e^{-iu}  + 1 + e^{iu}) =\frac{1}{3}(\cos (-u)+i \sin (-u)+1+\cos u+i \sin u)=\\ \frac{1}{3}(\cos u-i \sin u+1+\cos u+i \sin u)=\frac{1}{3}(1+2 \cos u)
\end{aligned}
\end{equation}

Действительнозначная!


\subsection{Задача 2}

$ \E N_t = 100t $

$ Y_i \sim exp(\frac{1}{5000}) $ - размер выплаты

$ \E Y_1  = 5000$

\subsubsection{i}

$ X_t = \sum_{0}^{N_t} Y_i $ - Составной процесс Пуассона, предполагая независимость $ \xi_i и N_t $. $ N_t $ - процесс Пуассона. $ \xi_1, \xi_2 - iid $

\subsubsection{ii}

\[ \E X_t = 100t \E Y_1 = 100t \cdot 5000 = 500000 \]

\[ \Var X_t = 100t \Var Y_1 = 100t \cdot 5000^2 + 5000^2 =5000000000 = 5t\cdot10^9  \]

\[ \P\{ X_t = 0 \} = \P\{ N_t = 0 \} = \frac{(100t)^0}{0!}e^{100t} = e^{-100t} \]

\[ \L_Y(u) = \int_{0}^{+\infty} \lambda e^{-\lambda x} e^{-ux}  = \int_{0}^{+\infty} \lambda e^{-(\lambda + u)x} = \lambda (- \frac{e^{-(\lambda + u)x}}{\lambda + u})\biggr|_0^{+\infty} = \frac{\lambda}{\lambda + u} \]

\[ \L_{X_t}  = e^{\lambda_1 t (\frac{\lambda_2}{\lambda_2 + u} - 1)} = e^{\frac{-\lambda_1 u t}{\lambda_2 + u}}  = e^\frac{-500000 u t}{1 + 5000u}\]

\subsection{Задача 3}
Ничтоже сумняшеся воспользуемсся формулами из четвёртого номера  и определений c семинара во славу Сатаны, конечно же. 

\subsubsection{i}

По 4(ii)
Однородный случай

$$ F_{S_{101} - S_{100}|S_{100} = 224}(t) = P(S_{101} - S_{100} \le t|S_{100} = 224) = 1 - e^{-0.1(t+224) + 10 \cdot 224} = 1 - e^{-10 t}$$

Неоднородный случай

$$ F_{S_{101} - S_{100}|S_{100} = 224}(t) = P(S_{101} - S_{100} \le t|S_{100} = 224) = 1 - e^{10(t + 224)^{\frac{5}{4}} + 10(224)^{\frac{5}{4}}}$$


\subsubsection{ii}
По семинару, разность считающих функций распределена по Пуассону. 

Однородный случай:

$$  F_{S_{101} - S_{100}|S_{100} = 224}(t) = P(N_{224+t} - N_{224} \ge 50) = 1 - \sum_{k=0}^{49} \frac{(10(224+t) - 10(224))^k}{k!} e^{-(10(224+t) - 10(224))} =$$

 $$ 1 - \sum_{k=0}^{49} \frac{(10t)^k}{k!} e^{-(10t)}$$
 
 Неоднородный случай:
 
 $$  F_{S_{101} - S_{100}|S_{100} = 224}(t) = P(N_{224+t} - N_{224} \ge 50) = 1 - \sum_{k=0}^{49} \frac{(10(224+t)^{\frac{5}{4}} - 10(224)^{\frac{5}{4}})^k}{k!} e^{-(10(224+t)^{\frac{5}{4}} - 10(224)^{\frac{5}{4}})} $$


\subsection{Задача 4}

$N_t$ -- неоднородный процесс Пуассона

$ N_t - N_s \sim Pois(\Lambda(t) - \Lambda(s))$\\[-6mm]

$N_t \sim Pois(\Lambda(t))$. $S_k = \min\{t: N_t = k\}; \xi_k~=~S_k~-~S_{k-1}$

По лекции:
$f_{\xi_1}(t) = \lambda(t) e^{-\Lambda(t)}$

\subsubsection{i}
По индукции:
\begin{enumerate}[\Sun]
	\item $n=1: f_{S_1}(t) = f_{\xi_1}(t) = e^{-\Lambda(t)} \frac{(\Lambda(t))^{1-1}}{(1-1)!} \lambda(t) = \lambda(t) e^{-\Lambda(t)}$ 

	
	\item Шаг индукции.
	
	 $\mathbb{P}(N_t = n) = \mathbb{P}(S_n \le t) - \mathbb{P}(S_{n+1} \le t) \Rightarrow F_{S_{n+1}}(t) = \mathbb{P}(S_{n+1} \le t) = \mathbb{P}(S_n \le t) - \mathbb{P}(N_t = n) = F_{S_n}(t) - e^{-\Lambda (t)} \frac{(\Lambda (t))^n}{n!}$
	$$f_{S_{n+1}}(t) = f_{S_n}(t) - \left(-\lambda(t) \cdot e^{-\Lambda(t)}  \frac{(\Lambda (t))^n}{n!} + e^{-\Lambda(t)} \frac{n(\Lambda(t))^{n-1} \cdot \lambda(t)}{n!}\right) =$$
	$$= e^{-\Lambda(t)} \frac{(\Lambda(t))^{n-1}}{(n-1)!} \lambda(t) + \lambda(t) \cdot e^{-\Lambda(t)}  \frac{(\Lambda (t))^n}{n!} - e^{-\Lambda(t)} \frac{n(\Lambda(t))^{n-1} \cdot \lambda(t)}{n!} =$$
	$$= \frac{n e^{-\Lambda(t)} (\Lambda(t))^{n-1} \lambda(t)}{n!} + \frac{e^{-\Lambda(t)} (\Lambda (t))^n \lambda(t)}{n!} - \frac{n e^{-\Lambda(t)} (\Lambda(t))^{n-1} \cdot \lambda(t)}{n!} = e^{-\Lambda(t)} \frac{(\Lambda (t))^n}{n!} \lambda(t)$$
\end{enumerate}

\subsubsection{ii} Если $\xi_{k+1} \le t$ то с момента $S_k$ и до $S_k + t$ произошел как минимум один момент восстановления. Может и больше, кто эти случайные процессы разберёт. Всё не как у людей. Тогда $N_{S_k + t} - N_{S_k} \ge 1$. Исходя из $S_k = s \Rightarrow N_{s + t} - N_{s} \ge 1$
$$\mathbb{P}(\xi_{k+1} \le t | S_k = s) = \mathbb{P}(N_{s+t} - N_s \ge 1) = 1 - \mathbb{P}(N_{s+t} - N_s < 1) = 1 - \mathbb{P}(N_{s+t} - N_s = 0) = 1 - e^{-\Lambda(t+s) + \Lambda(s)}$$


\subsubsection{iii}
 $$F_{\xi_k}(t) = \mathbb{P}(\xi_K \le t) = \int\limits_0^{+\infty} \mathbb{P}(\xi_k \le t | S_{k-1} = s) \cdot f_{S_{k-1}}(s) ds = \int\limits_0^{+\infty} (1 - e^{-\Lambda(t+s) + \Lambda(s)}) \cdot e^{-\Lambda(s)} \frac{(\Lambda (s))^{k-2}}{(k-2)!} \lambda(s) ds = $$
$$= \int\limits_0^{+\infty} e^{-\Lambda(s)} \frac{(\Lambda (s))^{k-2}}{(k-2)!} \lambda(s) - e^{-\Lambda(t+s) + \Lambda(s)} \cdot e^{-\Lambda(s)} \frac{(\Lambda (s))^{k-2}}{(k-2)!} \lambda(s) ds = \int\limits_0^{+\infty} f_{S_{k-1}}(s) - e^{-\Lambda(t+s)} \cdot \frac{(\Lambda (s))^{k-2}}{(k-2)!} \lambda(s) ds =$$
$$= 1 - \int\limits_0^{+\infty} e^{-\Lambda(t+s)} \cdot \frac{(\Lambda (s))^{k-2}}{(k-2)!} \lambda(s) ds $$

\section{Домашнее задание 5}

Для начала выпишем заготовку ответа.

\[ P\{ X_n = j | X_{n-1} = i_{n-1} \} = 
\begin{cases}
1  \text{ если } i_{n-1} < s, j = S \\
0  \text{ если } i_{n-1} < s, j \neq S\\
(\star) \text{ если } i_{n-1} \ge s, i_{n-1} \ge j\\
(\star \star) \text{ если } i_{n-1} \ge s, i_{n-1} < j
\end{cases}\]

Теперь кратко поясним полученную конструкцию. Функция распределения распадается на два случая. Если склад был в предыдущий день достаточно опустошён, то есть  $ i_{n-1} < s $, то очевидно, что значение заполнения склада в следующий день предопределено и с вероятностью 1 оно равно $S$ и с нулевой - чему-то иному. Также очевидно, что никакая предыстория не влияет на эту определённость. Последнее состояние полностью определяет будущее.

Ситуацию, когда склад был заполнен достаточно, нужно рассмотреть отдельно.

\[ (\star) = p\{ X_{n-1} - D_{n-1} | X_{n-1} = i_{n-1} \} = \P\{ i_{n-1} - D_{n-1}  = j \} = P\{ D_{n-1} = i_{n-1} - j \} \] 

При добавлении предыстории ничего не изменится, так как $ D_{n-1} \text{и} X_{n-2} $ независимы.
Очевидно, что заказ не может быть отрицательным, поэтому $ (\star \star) = 0 $. Так как в задании не указано, то в предыдущей опции предполагается, что заказ может быть нулевым.


\[ P\{ X_n = j | X_{n-1} = i_{n-1} \} = 
\begin{cases}
1  \text{ если } i_{n-1} < s, j = S \\
0  \text{ если } i_{n-1} < s, j \neq S\\
P\{ D_{n-1} = i_{n-1} - j \} \text{ если } i_{n-1} \ge s, i_{n-1} \ge j\\
0 \text{ если } i_{n-1} \ge s, i_{n-1} < j
\end{cases}\]

\subsection{Задача 2}


\begin{figure}[!h]
	\begin{center}
	\includegraphics[width=0.6\linewidth]{51}
	\caption{Графическое представление Марковской цепи}
	\label{graph}
\end{center}
\end{figure}


\subsubsection{i} 

Исходя из графа на Рис. \ref{graph}, можно выделить 3 класса эквивалентности: (4,5), (3), (1,2,7,6). Я не знаю, нужно ли тут ещё что-то пояснять. Просто по определению это классы эквивалентности. Попав из любого состояния в любое иное, можно вернуться назад.



\subsubsection{ii} 


\begin{table}[]
	\begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		\rowcolor[HTML]{9AFF99} 
		\textbf{Вершина} & \textbf{Существенность} & \textbf{Период} \\ \hline
		1                & Существенна             & 1               \\ \hline
		2                & Существенна             & 1               \\ \hline
		3                & Несущественна           & 1               \\ \hline
		4                & Существенна             & 2               \\ \hline
		5                & Существенна             & 2               \\ \hline
		6                & Существенна             & 1               \\ \hline
		7                & Существенна             & 1               \\ \hline
	\end{tabular}
\end{center}
\end{table}



Опять же, из графа очевидна существенность вершин. Вершины 4 и 5 существенны, так как между ними есть связь, а больше идти некуда. Вершина 3 несущественна, так как из неё можно перейти в 4 и не вернуться. Все остальные вершины существенны, так как лежат внутри одного класса эквивалентности и вершина 1 существенна. А как мы знаем, в классе эквивалентности если одна вершина существенна, то и все остальные тоже. 

Так как все вершины внутри класса эквивалентности имеют один период, то найдём период для вершины 1. Так как наличествуют пути 1-2-1 и 1-2-7-1, то НОД длин путей не может быть иным кроме 1. Следовательно, период вершин 1, 2, 6, и 7 равняется 1.

Период для вершины 3 равняется 1 по определению, так как в неё невозможно вернуться.

Периоды вершин 4 и 5, очевидно, 2, так как все возможные пути кратны 2. Все результаты представлены в таблице выше. 

\subsubsection{iii} 

Для этого нужно решить систему вида $ \pi P = \pi  $, где $ \pi_{1\times7} $

\[
P=\left(\begin{array}{ccccccc}{0} & {1 / 2} & {0} & {0} & {0} & {0} & {1 / 2} \\ {1 / 3} & {0} & {0} & {0} & {0} & {1 / 3} & {1 / 3} \\ {0} & {1 / 2} & {0} & {1 / 2} & {0} & {0} & {0} \\ {0} & {0} & {0} & {0} & {1} & {0} & {0} \\ {0} & {0} & {0} & {1} & {0} & {0} & {0} \\ {0} & {1 / 2} & {0} & {0} & {0} & {0} & {1 / 2} \\ {1 / 3} & {1 / 3} & {0} & {0} & {0} & {1 / 3} & {0}\end{array}\right)
\]

Дополнительно нужно ввести в систему условие $ \sum_{i=1}^7 \pi_i = 1$. Из системы можно выкинуть одно уравнение, как было показано на лекции. Выкинем второе, там больше всего коэффициентов. Система, очевидно, неэргодическая, и вполне можно ожидать неединственность решения. Решение системы несложное, но уж очень лень техать. Предложу поверить мне на слово.

\[ \begin{cases} 
\frac{1}{3}\pi_2 + \frac{1}{3}\pi_7 = \pi_1\\
0 = \pi_3\\
\frac{1}{2}\pi_3 + \pi_5 = \pi_4\\
\pi_4 = \pi_5\\
\frac{1}{3}\pi_2 + \frac{1}{3}\pi_7 = \pi_6\\
\frac{1}{2}\pi_1 + \frac{1}{3}\pi_2 + \frac{1}{2}\pi_6 = \pi_7\\
\pi_1 + \pi_2 + \pi_3 + \pi_4 + \pi_5 + \pi_6 + \pi_7 = 1
 \end{cases} \Rightarrow \begin{cases}
 \pi_1 = \pi_6\\
 \pi_2 = \pi_7\\
 \pi_3 = 0\\
 \pi_4 = \pi_5\\
 \pi_6 = \frac{2}{3}\pi_7\\
 \pi_7 = (1-2\pi_5)\frac{3}{10}
 \end{cases}\]



Например, одним из возможных распределений будет:

\[ \begin{cases}
\pi_1 = 0\\
\pi_2 = 0\\
\pi_3 = 0\\
\pi_4 = \frac{1}{2}\\
\pi_5 = \frac{1}{2}\\
\pi_6 = 0\\
\pi_7 = 0
\end{cases} \]

Наблюдаем то, что и должны. Неэргодическая система неустойчива к стартовому состоянию и, следовательно, $ \lim\limits_{n \to \infty} p_{ij}(n) \neq \pi_{j} $. Очевидно, что если стартовое стстояние будет в одной из точек (4,5), то в асимптотике распределение будет таким, как в решении выше, а если в одной из точек (1,2,6,7), то распределение будет совсем иным. Интересный результат, я, кажется, стал лучше понимать предпосылки эргодической теоремы.

\subsection{Задача 3}

Воспользуемся спектральным разложением матрицы из условия для возведения в степень.

\[
\left|\begin{array}{ccc}{-\lambda} & {1 / 2} & {1 / 2} \\ {1 / 3} & {1 / 3 - \lambda} & {1 / 3} \\ {1 / 4} & {1 / 2} & {1 / 4 - \lambda}\end{array}\right| = 0 \Rightarrow \text{Чёрное колдовство} \Rightarrow 24 \lambda^3 - 14\lambda^2 - 9\lambda - 1 = 0
\]

Как видим, корень уравнения $ \lambda = 1 $ подходит, так что всё хорошо. Поделим многочлен на многочлен и получим:


\[ \frac{24 \lambda^3 - 14\lambda^2 - 9\lambda - 1}{\lambda - 1} = 24\lambda^2 + 10\lambda + 1 \]

Решим уравнение $ 24\lambda^2 + 10\lambda + 1 $

\[ D = 100 - 96 = 4 \]

\[ \lambda_{2}  = -\frac{1}{4}\]
\[ \lambda_{3}  = -\frac{1}{6}\]

Так как элементы матрицы в степени n это какие-то линейные комбинации собственных значений. Выпишем через первые несколько степеней значения линейных комбинаций, чтобы получить систему:

\[ p^n_{23} = A + B \left(-\frac{1}{4}\right)^n + C\left(-\frac{1}{6}\right)^n\]

\[ \begin{cases}
A + B + C = 0\\
A - \frac{1}{4}B - \frac{1}{6}C = \frac{1}{3}\\
A + \frac{1}{16}B + \frac{1}{36}C = \frac{13}{36}
\end{cases} \Rightarrow \begin{cases}
A + B + C = 0\\
12A - 3B - 2C = 4\\
144A + 9B + 4C = 52
\end{cases} \Rightarrow  \]

\[ \Rightarrow \text{Чёрное колдовство методом Крамера} \Rightarrow \begin{cases}
A = \frac{12}{35}\\
B = \frac{4}{5}\\
C = -\frac{8}{7}
\end{cases}\]

Ответ:

\[ p^n_{23} = \frac{12}{35} + \frac{4}{5} \left(-\frac{1}{4}\right)^n  -\frac{8}{7} \left(-\frac{1}{6}\right)^n\]


\subsection{Задача 4}

Обозначим два возможных состояниа: A - победа, B - поражение. Составим матрицу переходов:

\[ P = \begin{pmatrix}
0.7 & 0.3 \\
0.6 & 0.4
\end{pmatrix} \]


Для Марковской цепи мы помним, что $ P^{(m)} = P^m$, так что без тени сомнений возведём матрицу в третью степень. Именно в третью, так как матрица в первой степени показывает вероятности исходов второй игры, во второй степени - третьей игры, а в третьей - четвёртой.

\[ P^3 = \begin{pmatrix}
0.667 & 0.333 \\
0.666 & 0.334
\end{pmatrix} \]

Следовательно, вероятность того, что выиграв первый матч, Крылья Советов победят вероятностью 0.667. Что-то меня терзают сомнения, что задача такая лёгкая.

\end{document}
